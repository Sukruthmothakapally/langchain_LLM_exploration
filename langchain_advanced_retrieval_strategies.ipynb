{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0689733d",
      "metadata": {
        "id": "0689733d"
      },
      "source": [
        "# Advanced Retrieval Techniques in LangChain with Vector Databases and Compression\n",
        "This notebook demonstrates advanced document retrieval techniques using LangChain, including similarity search, maximum marginal relevance (MMR), metadata filtering, self-querying with LLMs, contextual compression, and comparison with traditional methods like TF-IDF and SVM. Example documents are embedded into ChromaDB and retrieved using a variety of retrieval strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed2569c6",
      "metadata": {
        "id": "ed2569c6"
      },
      "source": [
        "## Vectorstore retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c18f8a7b-62af-403e-9877-18d1c2265b4f",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c18f8a7b-62af-403e-9877-18d1c2265b4f"
      },
      "outputs": [],
      "source": [
        "# !pip install lark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain_community"
      ],
      "metadata": {
        "id": "_FxdCtTOTKPG"
      },
      "id": "_FxdCtTOTKPG",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-openai"
      ],
      "metadata": {
        "id": "O5RvaUqaX9dk"
      },
      "id": "O5RvaUqaX9dk",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c2d552e1",
      "metadata": {
        "id": "c2d552e1"
      },
      "source": [
        "### Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "fe368042",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "fe368042"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "persist_dir = 'docs/chroma/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'xxx'"
      ],
      "metadata": {
        "id": "YsZg4uJVTsnM"
      },
      "id": "YsZg4uJVTsnM",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install chromadb"
      ],
      "metadata": {
        "id": "_rouIwAaTwwL"
      },
      "id": "_rouIwAaTwwL",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"The Transformer architecture has fundamentally reshaped natural language processing, enabling models such as BERT, GPT, and T5 to excel in tasks like translation, summarization, and question answering.\",\n",
        "    \"In financial modeling, Monte Carlo simulations are used to understand the impact of risk and uncertainty by simulating a wide range of possible outcomes.\",\n",
        "    \"The CRISPR-Cas9 gene-editing technology has revolutionized genetic engineering, allowing scientists to make precise modifications to DNA in living organisms.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "d95ZdgJwYrJn"
      },
      "id": "d95ZdgJwYrJn",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_texts(\n",
        "    texts=documents,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_dir\n",
        ")\n",
        "vectordb.persist()\n",
        "embedding = OpenAIEmbeddings()\n",
        "persist_directory = 'docs/chroma/'"
      ],
      "metadata": {
        "id": "6jNJSM0uYwAt"
      },
      "id": "6jNJSM0uYwAt",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "3659e0f7",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3659e0f7",
        "outputId": "fcbed719-6ebd-4c63-99fc-d2b354a70138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "a807c758",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "a807c758"
      },
      "outputs": [],
      "source": [
        "science_corpus = [\n",
        "    \"Quantum entanglement is a physical phenomenon where particles remain interconnected, sharing physical states even when separated by vast distances.\",\n",
        "    \"A white dwarf is a stellar remnant composed mostly of electron-degenerate matter and forms when a low-mass star has exhausted all its central nuclear fuel.\",\n",
        "    \"Schrödinger's cat is a thought experiment that illustrates the paradox of quantum superposition.\"\n",
        "]\n",
        "science_vectordb = Chroma.from_texts(science_corpus, embedding=embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tiktoken"
      ],
      "metadata": {
        "id": "JXg8Cg1OT9a-"
      },
      "id": "JXg8Cg1OT9a-",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "9a37b5a5",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a37b5a5",
        "outputId": "f748b3e5-9522-4558-c2d0-eca9f7933901"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='A white dwarf is a stellar remnant composed mostly of electron-degenerate matter and forms when a low-mass star has exhausted all its central nuclear fuel.'),\n",
              " Document(metadata={}, page_content='Quantum entanglement is a physical phenomenon where particles remain interconnected, sharing physical states even when separated by vast distances.')]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "query = \"What happens to stars when they die?\"\n",
        "science_vectordb.similarity_search(query, k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "24e3b025",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24e3b025",
        "outputId": "06adcfcc-5ca1-4e3c-ea18-e2f14a655513"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='A white dwarf is a stellar remnant composed mostly of electron-degenerate matter and forms when a low-mass star has exhausted all its central nuclear fuel.'),\n",
              " Document(metadata={}, page_content=\"Schrödinger's cat is a thought experiment that illustrates the paradox of quantum superposition.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "science_vectordb.max_marginal_relevance_search(query, k=2, fetch_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a29e8c9",
      "metadata": {
        "id": "5a29e8c9"
      },
      "source": [
        "### Addressing Diversity: Maximum marginal relevance\n",
        "\n",
        "`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "9bb2c0a9",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "9bb2c0a9"
      },
      "outputs": [],
      "source": [
        "query = \"Explain the benefits of transformer-based models.\"\n",
        "results = vectordb.similarity_search(query, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "f07f8793",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "f07f8793",
        "outputId": "d5bc7ba0-4aeb-4796-c0a8-689027f01dbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Transformer architecture has fundamentally reshaped natural language processing, enabling models such as BERT, GPT, and T5 to excel in tasks like translation, summarization, and question answering.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "results[0].page_content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "e9f7e165",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e9f7e165",
        "outputId": "976e06ab-c2ed-4ca4-9b5c-40c282c64f5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In financial modeling, Monte Carlo simulations are used to understand the impact of risk and uncerta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "results[1].page_content[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4ca1b6",
      "metadata": {
        "id": "4c4ca1b6"
      },
      "source": [
        "Note the difference in results with `MMR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "222234c5",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "222234c5",
        "outputId": "8e5ab6bb-6eb5-4d3e-84af-a7697cda28d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n"
          ]
        }
      ],
      "source": [
        "mmr_results = vectordb.max_marginal_relevance_search(query, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "93b20226",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "93b20226",
        "outputId": "e0678614-a4f3-40a8-e833-ff195e8a2f3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Transformer architecture has fundamentally reshaped natural language processing, enabling models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "mmr_results[0].page_content[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "17d39762",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "17d39762",
        "outputId": "53b74547-c6cd-44a9-f595-924eeda92656"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In financial modeling, Monte Carlo simulations are used to understand the impact of risk and uncerta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "mmr_results[1].page_content[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b909bc",
      "metadata": {
        "id": "b2b909bc"
      },
      "source": [
        "### Addressing Specificity: working with metadata\n",
        "\n",
        "To address this, many vectorstores support operations on `metadata`.\n",
        "\n",
        "`metadata` provides context for each embedded chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "3c1a60b2",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "3c1a60b2"
      },
      "outputs": [],
      "source": [
        "query = \"What were the applications of SVMs discussed in Lecture 3?\"\n",
        "filtered_docs = vectordb.similarity_search(\n",
        "    query,\n",
        "    k=3,\n",
        "    filter={\"source\": \"docs/ai_course/lecture3.pdf\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "a8612840",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "a8612840"
      },
      "outputs": [],
      "source": [
        "for doc in filtered_docs:\n",
        "    print(doc.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "2708f6ae",
      "metadata": {
        "height": 30,
        "id": "2708f6ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ccc2d784",
      "metadata": {
        "id": "ccc2d784"
      },
      "source": [
        "### Addressing Specificity: working with metadata using self-query retriever\n",
        "\n",
        "\n",
        "To address this, use `SelfQueryRetriever`, which uses an LLM to extract:\n",
        "\n",
        "1. The `query` string to use for vector search\n",
        "2. A metadata filter to pass in as well\n",
        "\n",
        "Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "b1d06084",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "b1d06084"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "0aa5e698",
      "metadata": {
        "height": 232,
        "tags": [],
        "id": "0aa5e698"
      },
      "outputs": [],
      "source": [
        "metadata_fields = [\n",
        "    AttributeInfo(name=\"source\", description=\"Source file path\", type=\"string\"),\n",
        "    AttributeInfo(name=\"page\", description=\"Page number within source\", type=\"integer\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "fc9de693-7bdb-463e-b124-9f72163b0bd8",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "fc9de693-7bdb-463e-b124-9f72163b0bd8"
      },
      "outputs": [],
      "source": [
        "document_content_description = \"AI research papers and lecture notes\"\n",
        "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
        "self_query_retriever = SelfQueryRetriever.from_llm(\n",
        "    llm_model,\n",
        "    vectordb,\n",
        "    document_content_description,\n",
        "    metadata_field_info=metadata_fields,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "79d781b9",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "79d781b9"
      },
      "outputs": [],
      "source": [
        "query = \"Did Lecture 2 mention linear regression in any detail?\"\n",
        "retrieved_docs = self_query_retriever.get_relevant_documents(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c51778b0-1fcd-40a4-bd6b-0f13fec8acb1",
      "metadata": {
        "id": "c51778b0-1fcd-40a4-bd6b-0f13fec8acb1"
      },
      "source": [
        "**You will receive a warning** about predict_and_parse being deprecated the first time you executing the next line. This can be safely ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "db04374e",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "db04374e"
      },
      "outputs": [],
      "source": [
        "for doc in retrieved_docs:\n",
        "    print(doc.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "297b8168",
      "metadata": {
        "id": "297b8168"
      },
      "source": [
        "### Additional tricks: compression\n",
        "\n",
        "Another approach for improving the quality of retrieved docs is compression.\n",
        "\n",
        "Information most relevant to a query may be buried in a document with a lot of irrelevant text.\n",
        "\n",
        "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
        "\n",
        "Contextual compression is meant to fix this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "a060cf74",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "a060cf74"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "038649c8",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "038649c8"
      },
      "outputs": [],
      "source": [
        "def pretty_print_docs(docs):\n",
        "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + doc.page_content for i, doc in enumerate(docs)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "fc686cf2",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "fc686cf2"
      },
      "outputs": [],
      "source": [
        "compressor = LLMChainExtractor.from_llm(llm_model)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "cde86848",
      "metadata": {
        "height": 64,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cde86848",
        "outputId": "812647bc-81d6-4f52-885a-2184da56133e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\n",
            "The Transformer architecture, models such as BERT, GPT, and T5, tasks like translation, summarization, and question answering.\n"
          ]
        }
      ],
      "source": [
        "query = \"How are GPT models used in summarization?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c4fc4d",
      "metadata": {
        "id": "82c4fc4d"
      },
      "source": [
        "## Combining various techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "161ae1ad",
      "metadata": {
        "height": 81,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "161ae1ad",
        "outputId": "9c98a785-ef38-48de-bbc9-9f20a738a449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\n",
            "The Transformer architecture, models such as BERT, GPT, and T5, tasks like translation, summarization, and question answering.\n"
          ]
        }
      ],
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
        ")\n",
        "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2b63e1",
      "metadata": {
        "id": "6c2b63e1"
      },
      "source": [
        "## Other types of retrieval\n",
        "\n",
        "It's worth noting that vectordb as not the only kind of tool to retrieve documents.\n",
        "\n",
        "The `LangChain` retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "83d2e808",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "83d2e808"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import SVMRetriever, TFIDFRetriever\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pypdf"
      ],
      "metadata": {
        "id": "pXy2RGMWZtwg"
      },
      "id": "pXy2RGMWZtwg",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "bcf5b760",
      "metadata": {
        "height": 183,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcf5b760",
        "outputId": "bbd2357c-346a-46f0-be59-1625f3b418a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n"
          ]
        }
      ],
      "source": [
        "loader = PyPDFLoader(\"/content/sample_data/sample-local-pdf.pdf\")\n",
        "pages = loader.load()\n",
        "full_text = \" \".join([p.page_content for p in pages])\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
        "pdf_chunks = splitter.split_text(full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "9bb0d781",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "9bb0d781"
      },
      "outputs": [],
      "source": [
        "svm_retriever = SVMRetriever.from_texts(pdf_chunks, embedding_model)\n",
        "tfidf_retriever = TFIDFRetriever.from_texts(pdf_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "0b1cc35f",
      "metadata": {
        "height": 64,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b1cc35f",
        "outputId": "54ca02f3-5e16-49da-e4ef-88891b8d918a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='1')"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "question = \"How long is this\"\n",
        "docs_svm=svm_retriever.get_relevant_documents(question)\n",
        "docs_svm[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "2a1659c0",
      "metadata": {
        "height": 64,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a1659c0",
        "outputId": "72280111-ce82-4636-adcf-a65daeb02d7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh.   Quisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. Nunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.   Suspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. Quisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue eget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Morbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue 3')"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "question = \"Quisque volutpat condimentum velit\"\n",
        "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
        "docs_tfidf[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7885389e",
      "metadata": {
        "height": 30,
        "id": "7885389e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}